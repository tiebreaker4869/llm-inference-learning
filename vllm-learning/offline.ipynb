{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9129f86-a41d-48da-8dba-27b37f876c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb2677c9-2550-4788-9024-4e8865874850",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"what is the highest mountain in China?\",\n",
    "    \"1 + 1 = ?\",\n",
    "    \"How to say hello in Japanese?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a84839d9-2d33-429f-9382-13dbe5449182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-12 00:31:47 [utils.py:253] non-default args: {'seed': None, 'disable_log_stats': True, 'model': 'Qwen/Qwen2.5-7B-Instruct'}\n",
      "WARNING 12-12 00:31:47 [arg_utils.py:1175] `seed=None` is equivalent to `seed=0` in V1 Engine. You will no longer be allowed to pass `None` in v0.13.\n",
      "INFO 12-12 00:31:47 [model.py:637] Resolved architecture: Qwen2ForCausalLM\n",
      "INFO 12-12 00:31:47 [model.py:1750] Using max model len 32768\n",
      "INFO 12-12 00:31:50 [scheduler.py:228] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=6137)\u001b[0;0m INFO 12-12 00:31:50 [core.py:93] Initializing a V1 LLM engine (v0.12.0) with config: model='Qwen/Qwen2.5-7B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01), seed=0, served_model_name=Qwen/Qwen2.5-7B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>}, 'local_cache_dir': None}\n",
      "\u001b[0;36m(EngineCore_DP0 pid=6137)\u001b[0;0m INFO 12-12 00:31:51 [parallel_state.py:1200] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.16.32.2:48383 backend=nccl\n",
      "\u001b[0;36m(EngineCore_DP0 pid=6137)\u001b[0;0m INFO 12-12 00:31:51 [parallel_state.py:1408] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[0;36m(EngineCore_DP0 pid=6137)\u001b[0;0m INFO 12-12 00:31:52 [gpu_model_runner.py:3467] Starting to load model Qwen/Qwen2.5-7B-Instruct...\n",
      "\u001b[0;36m(EngineCore_DP0 pid=6137)\u001b[0;0m INFO 12-12 00:31:52 [cuda.py:411] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76a5820cb13140dc92f29820c0169a90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/3.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3538c9fd507484c99bf5952ffc57e42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b28dbaac612a4039bc41746c7be7929f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9749076cdc943bd80fedb77f702d567",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/3.56G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=6137)\u001b[0;0m INFO 12-12 00:34:53 [weight_utils.py:487] Time spent downloading weights for Qwen/Qwen2.5-7B-Instruct: 180.281405 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d09ede9eb264801847d1242ed303437",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "510c73d0f53c4aaebafe8f344f1122b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=6137)\u001b[0;0m INFO 12-12 00:35:01 [default_loader.py:308] Loading weights took 8.10 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=6137)\u001b[0;0m INFO 12-12 00:35:02 [gpu_model_runner.py:3549] Model loading took 14.2488 GiB memory and 189.074724 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=6137)\u001b[0;0m INFO 12-12 00:35:08 [backends.py:655] Using cache directory: /root/.cache/vllm/torch_compile_cache/1d3f319a96/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[0;36m(EngineCore_DP0 pid=6137)\u001b[0;0m INFO 12-12 00:35:08 [backends.py:715] Dynamo bytecode transform time: 5.56 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=6137)\u001b[0;0m INFO 12-12 00:35:12 [backends.py:257] Cache the graph for dynamic shape for later use\n",
      "\u001b[0;36m(EngineCore_DP0 pid=6137)\u001b[0;0m INFO 12-12 00:35:18 [backends.py:288] Compiling a graph for dynamic shape takes 9.85 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=6137)\u001b[0;0m INFO 12-12 00:35:20 [monitor.py:34] torch.compile takes 15.41 s in total\n",
      "\u001b[0;36m(EngineCore_DP0 pid=6137)\u001b[0;0m INFO 12-12 00:35:21 [gpu_worker.py:359] Available KV cache memory: 24.26 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=6137)\u001b[0;0m INFO 12-12 00:35:22 [kv_cache_utils.py:1286] GPU KV cache size: 454,240 tokens\n",
      "\u001b[0;36m(EngineCore_DP0 pid=6137)\u001b[0;0m INFO 12-12 00:35:22 [kv_cache_utils.py:1291] Maximum concurrency for 32,768 tokens per request: 13.86x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:03<00:00, 16.66it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:01<00:00, 20.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=6137)\u001b[0;0m INFO 12-12 00:35:27 [gpu_model_runner.py:4466] Graph capturing finished in 5 secs, took 0.54 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=6137)\u001b[0;0m INFO 12-12 00:35:27 [core.py:254] init engine (profile, create kv cache, warmup model) took 25.31 seconds\n",
      "INFO 12-12 00:35:28 [llm.py:343] Supported tasks: ['generate']\n",
      "ERROR 12-12 00:38:14 [core_client.py:600] Engine core proc EngineCore_DP0 died unexpectedly, shutting down client.\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(model=\"Qwen/Qwen2.5-7B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb9f52f4-12cc-43a6-afac-c8b12132cf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(temperature=0.7, top_p=0.8, max_tokens=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "373ddce2-7a3c-4944-a892-d668c3f8a3c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e349044198b40c2abe68c3ef2d3822a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e91df92902e841ecb48fd8de8ffa215c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "outputs = llm.generate(prompts, sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e255046f-e761-4323-998f-d1cd749d78cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer 1:\n",
      " The highest mountain in China is Mount Everest, which has an elevation of 8,848 meters (29,029 feet) above sea level. However, it is important to note that Mount Everest is located on the border between Nepal and Tibet (China). The highest mountain that is entirely within the borders of China is Mount K2, also known as Mount Godwin-Austen, which has an elevation of 8,611 meters (28,251 feet) above sea level. K2 is located in the Karakoram range in the western part of China's Xinjiang Uygur Autonomous Region. \n",
      "\n",
      "If you are looking for the highest mountain peak in the Chinese part of the Himalayas, then Mount Everest is the answer. If you are looking for the highest mountain peak that is entirely within the borders of China, then Mount K2 is the answer. \n",
      "\n",
      "So, to be precise:\n",
      "- Mount Everest (8,848 meters) - on the border between China and Nepal.\n",
      "- Mount K2 (8,611 meters) - entirely within China (Xinjiang Uygur Autonomous Region). \n",
      "\n",
      "Both are among the highest mountains in the world, but they are located in different parts of\n",
      "answer 2:\n",
      " Let's solve the problem step by step:\n",
      "\n",
      "1. Start with the expression: \\(1 + 1\\).\n",
      "2. Add the numbers together: \\(1 + 1 = 2\\).\n",
      "\n",
      "So, the answer is \\(\\boxed{2}\\).\n",
      "answer 3:\n",
      " In Japanese, you can say \"hello\" in a few different ways depending on the context and the level of formality. Here are some common ways to greet someone in Japanese:\n",
      "\n",
      "1. **こんにちは (Konnichiwa)**: This is the most common and versatile greeting, used throughout the day. It's appropriate in most situations, from casual to slightly formal.\n",
      "\n",
      "2. **おはようございます (Ohayou gozaimasu)**: This means \"good morning\" and is used from early morning until around noon.\n",
      "\n",
      "3. **お昼は (Ozoo wa)**: This is a less formal way to say \"good noon\" and is used around lunchtime.\n",
      "\n",
      "4. **こんにちは (Konnichiwa) / おはようございます (Ohayou gozaimasu)**: These can also be used to say \"good evening\" in certain contexts, especially in more casual settings.\n",
      "\n",
      "5. **こんばんは (Konbanwa)**: This means \"good evening\" and is used from late afternoon until bedtime.\n",
      "\n",
      "6. **お元気ですか (O-genki desu ka)**: This means \"How are you?\" and is a more formal way to greet someone, often used in business or formal settings.\n",
      "\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "for idx, output in enumerate(outputs, start = 1):\n",
    "    print(f\"answer {idx}:\")\n",
    "    print(output.outputs[0].text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
